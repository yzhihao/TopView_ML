# 线性代数 SVD 线性变换

## 对称矩阵

### 性质

对称矩阵顾名思义，即转置等于它本身的矩阵，它的两个特殊性质，是尤为重要的：
1. **特征值为实数**
2. **特征向量相互正交**

通常情况下，一个矩阵可以进行如下分解：
$$$
A=S\bigwedge S^{-1}
$$$
那么，在矩阵对称的情况下，由性质2我们可以得知，当我们把矩阵的特征向量化为长度为1的向量时，就可以得到一组标准的正交向量。所以对于对称矩阵A有：
$$$
A=Q\bigwedge Q^{-1}，其中因为对于标准正交矩阵Q，有Q^{-1}=Q^{T}，所以公式还可以化为：\\
A=Q\bigwedge Q^{T}
$$$

对于这个公式，我们可以将它转化成线性方程：
$$$
A=Q\bigwedge Q^{T}=
\left[
\begin{matrix}
	q_1&q_2&...&q_n
\end{matrix}
\right]
\left[
\begin{matrix}
	λ_1&&&&\\
    &λ_2&&\\
    &&...&\\
    &&&λ_n
\end{matrix}
\right]
\left[
\begin{matrix}
	q_1^{T}\\
    q_2^{T}\\
    ...\\
    q_n^{T}
\end{matrix}
\right]=
$$$
$$$
λ_1q_1q_1^{T}+λ_2q_2q_2^{T}+...+λ_nq_nq_n^{T}
$$$
$$$
由于Q是标准正交矩阵，所以q_1,q_2...是单位列向量，因此q_1^{T}q_1=1，由此得到\\
\frac{qq^T}{q^Tq}=q(q^Tq)^{-1}q^T=qq^T，即qq^T是一个投影矩阵
$$$
由此我们可以得到，**每一个对称矩阵都可以分解为一系列相互正交的投影矩阵**

对称矩阵还有一个性质，即：
* **主元符号的正负数量与特征向量的正负数量相同**

## 正定矩阵

### 性质

符合以下任意一个条件的矩阵为正定矩阵：
1. 矩阵的**所有特征值大于零**
2. 矩阵的**所有顺序主子阵的行列式（即顺序主子式）大于零**
3. 矩阵消元后**主元均大于零**
4. **xTAx>0**

前三条大多用于证明正定性，而第四条用于定义正定性

备注：
假设有一个二维矩阵
$$$
\left[
\begin{matrix}
	2&6\\
    6&18
\end{matrix}
\right]
$$$
则它的**顺序主子式的值**为矩阵$$$\left[\begin{matrix}2\end{matrix}\right]$$$的行列式与矩阵$$$\left[\begin{matrix}2&6\\6&18\end{matrix}\right]$$$的行列式的值，即 2 与 0，发现有一个不大于零，所以该矩阵不是正定矩阵。
它的**第一个主元**为从左往右从上往下第一个非零的数，这里为2，**第二个主元为该矩阵的二阶顺序主子式除以第一个主元**，这里没有（因为**主元必须非零**），**第三个主元为该矩阵的三阶顺序主子式除以第一个主元**...以此类推，此例中的矩阵仅有一个主元。

这个例子所举的矩阵被称为“**半正定矩阵**”，矩阵为**半正定矩阵的条件就是把正定矩阵的四个条件中“大于0”的部分改为“不小于0”**

$$$x^TAx$$$
x可以是**任意向量**。

## 奇异值分解（SVD）

### 定义

$$$
将一个奇异矩阵A分解为UΣV^T即为奇异值分解，即:\\
A=UΣV^T\\
其中U和V为通常不同的两个正交矩阵，Σ为对角矩阵
$$$

### 意义

SVD可以视为对一个奇异矩阵的“特征分解”（事实上我们知道只有可逆矩阵才能进行特征分解），或者从另一种角度说，它让我们能够对所有的矩阵进行原本只有可逆矩阵才能进行的对矩阵特征的提取。

### 推导

$$$
我们先从矩阵A的列空间中找出一组特殊的正交基，设为v_1,v_2,...v_r，\\
再从矩阵的行空间中找出一组特殊的正交基，设为w_1,w_2,...w_r
$$$
$$$
我们设正交基v_1,v_2,...v作为列向量组成的矩阵为v，w_1,w_2,...作为列向量组成的矩阵为w，则我们有：
$$$
$$$
Av=σw\\
展开为Av_1=σ_1w_1,Av_2=σ_2w_2...\\
σ=
\left[
\begin{matrix}
	σ_1&&&0\\
    &σ_2\\
    &&...\\
    0&&&σ_r\\
    &&&&...\\
    &&&&&0
\end{matrix}
\right]\\
是缩放因子σ_1,σ_2...组成的对角矩阵，它表示进行矩阵A表示的线性变换的时候，向量受到的拉伸或者压缩量\\
$$$
实际上，我们可以将这个式子看作是A的列空间在A的行空间上的一个映射关系，因为我们可以看到矩阵A的列空间的正交基被映射在了矩阵A的行空间中，并被矩阵A在行空间的正交基所表示，而那些0值，则体现了矩阵A的零空间和左零空间。因此，当我们在矩阵A的列空间中有一个用基向量组v表示的向量的时候，我们就可以将其通过这个式子映射到A的行空间中用基向量w所表示的向量。

最终我们得到了式子
$$$
AV=UΣ\\
也即是SVD分解公式：A=UΣV^{-1}=UΣV^{T}
$$$

### 计算

要计算SVD，我们需要这样做：
首先，我们知道$$$A^{T}A$$$为一个对称正定（或者半正定）矩阵，那么
$$$
A^{T}A=V\bigwedge V^{T}\\
这个式子我们可以直接视为对矩阵A^{T}A进行特征值分解，那么V就是矩阵的特征向量矩阵，\bigwedge就是矩阵的对角特征值矩阵
$$$
$$$
而且这个特征向量矩阵是单位由特征向量组成的，要将特征向量标准化，只需要用它除以它的长度\\
并且\bigwedge=Σ^2 也就是说，Σ应为矩阵A^TA的特征值矩阵的二次方根
$$$
$$$
得出ΣV^{T}之后，只需要回带求出U即可，
$$$
$$$
或者我们也可以利用矩阵AA^T=UΣ^2U^{T}来对U进行计算，但要注意的是，\\
如果我们先求出了V，就需要确保我们求出的U能够与V搭配使得式子A=UΣV^{T}成立。
$$$
$$$
先求V或者先求U实际上都是可以的，因为结果并不会因此改变。
$$$

## 线性变换

饶是前一篇笔记中有讲到关于线性变换的知识，但是那是基于几何意义上的理解，并且仅适用于在二维平面上进行理解，这次的线性变换，则是从广义上对其进行理解

### 定义

$$$
设T为一个变换，则若对T输入向量v可以得到向量v经T变换之后输出的向量T(v)
$$$
那么若T为线性变换，那它必须满足如下两个性质：
$$$
T(v+w)=T(v)+T(w)\\
T(cv)=cT(v)
$$$
换句话说，我们可以将其看作是一条式子:
$$$
T(cv+dw)=cT(v)+dT(w)
$$$

### 计算

计算线性变换矩阵A的方法：
1. $$$确定输入空间的基v_1,v_2...与输出空间的基w_1,w_2...$$$
2. $$$计算T(v_1)=a_{11}w_1+a_{21}w_2+...得出系数a_{i1}为矩阵A的第一列，\\然后计算T(v_2),T(v_3)...$$$
3. 以此类推得到完整矩阵A

备注：
求导也可以认为是一种从高维到低维的线性变换。

矩阵的逆相当于对应线性变换的逆运算，矩阵的乘积相当于线性变换的乘积，矩阵的乘法也是源自于线性变换。
