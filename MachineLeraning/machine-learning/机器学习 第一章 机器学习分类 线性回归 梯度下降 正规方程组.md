# 机器学习 第一章 机器学习分类 线性回归 梯度下降 正规方程组

参考资料来源于：
[Machine learning: Trends, perspectives, and prospects](http://m.blog.csdn.net/Yan_Joy/article/details/53191669)
[斯坦福大学机器学习CS 229公开课第一讲讲义](http://cs229.stanford.edu/notes/cs229-notes1.pdf)
[梯度下降(Gradient Descent)小结 - 刘建平Pinard - 博客园](http://www.cnblogs.com/pinard/p/5970503.html)
[梯度下降算法和正规方程组学习笔记 - CODE and POEM - CSDN博客](http://blog.csdn.net/hahajinbu/article/details/49904665)

## 机器学习简介

### 定义

**对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E学习。**

### 分类

我们倾向于将机器学习中的学习形式分为以下几类，监督学习，无监督学习，强化学习

#### 监督学习

表示已经有标准输入（特征）和标准输出（目标），即已知样本数据的标准答案，提供对错提示，让机器能够从中获取信息从而使它能够在其它同类样本中达到分类和预测的目的。

举个例子，我们想让一辆汽车能够自动驾驶，那么有这么一种方法让其获得自动驾驶的能力，先人为地替汽车在固定路线上（训练集）驾驶，这是标准输入，即标准的特征，然后机器经过一段时间从中获得对行驶路线的判断能力，从而能够在其它路线上（测试集）自动行驶，这就是标准输出，即目的。也就是说，我们知道汽车应该怎么做，然后引导汽车向我们实现我们希望的目标。

监督学习常被用来解决回归问题和分类问题。

#### 无监督学习

无监督学习表示我们并不知道既定的标准答案，需要机器自行从一堆数据中通过分析数据的结构来得到这个数据的特征，然后分别聚类。

例如有几十则新闻，我们需要机器来对这几十则新闻的信息进行检索，然后找出不同的新闻与新闻之间的联系，从而归纳出他们的特征，以达到给这些新闻分类的目的。

无监督学习常被用于处理聚类问题。

#### 强化学习

在有些分类中机器学习仅分为监督学习和无监督学习，但也有些分类方式会给机器学习分出第三种类别，也即强化学习。

如果Agent的某个行为策略导致环境正的奖赏(强化信号)，那么Agent以后产生这个行为策略的趋势便会加强。Agent的目标是在每个离散状态发现最优策略以使期望的折扣奖赏和最大。
来源：[Machine learning: Trends, perspectives, and prospects](http://m.blog.csdn.net/Yan_Joy/article/details/53191669)

它与监督学习不同的地方在于，强化学习中，机器会先做出动作（当然也可能会已经接受过标准输入信号），然后根据自己所做出的动作得到来自环境的强化信号（奖或惩），然后机器根据当前状态再选择下一个动作。

我们依然可以用自动行驶的汽车来举例，当汽车自动行驶时，它撞上了障碍物，那么它就会接收到一个来自环境的“坏”信号，从而让它对于“撞上障碍物”这件事情得到一个负面的反馈，下一次就会减少撞上障碍物的行为；同理，当它成功绕过一个障碍物，就会接受正信号，以增加下次再遇到障碍物时选择绕过障碍物的几率。

## 常用符号

我们需要先了解几个在机器学习中常常会用到的符号：

m：训练样本的数目，例如判断一个肿瘤是良性还是恶性的数据分析中，大量未知其良性还是恶性的样本肿瘤的总数就是该训练样本的数目。

$$x：输入变量（特征）$$

$$y：输出变量（目标变量）$$

$$(x,y)：一个样本$$

$$(x^{(i)},y^{(i)})：第i个样本$$

$$θ(parameter)：参数，一般为我们希望得到的目标参数$$

$$h：输出函数，也经常被称之为“假设”（hypothesis）$$

$$a:=b：表示在运算中a被b的值覆盖$$

$$a=b：真值断言，就是a的值等于b$$

$$\{(x^{(i)},y^{(i)})i=1,...,m;\}：训练集（training set）$$

## 线性回归

为了引入线性回归，我们需要先引入一个用于描述监督学习的流程：

$$
我们的目标是，给定一个训练集X，让机器学习到这映射h:X -> Y ，从而让它得知h(x)是对应y的一个“好的”预测或者说是自变量，我们可以将这个过程用下面这副图来表示：
$$

![](img\supervise-learning.png)
[图片来源:斯坦福大学机器学习CS 229公开课第一讲讲义](http://cs229.stanford.edu/notes/cs229-notes1.pdf)

当我们要预测的值是**连续**的时候，我们就称这个学习问题是一个**回归问题**（regression problem）。当我们试图预测的值是一个**离散值**的时候（例如我们试图让机器判断一个住所是单元房还是公寓的时候，机器仅从“单元房”和“公寓”取y值），我们就称之为**分类问题**(classification problem)。

### 例子

为了更加方便的说明线性回归算法，我们需要先引入一个题目。

假设有一片住户区，那里有面积大小和卧室数量不尽相同不同的房子，如图：

![](img\supervise-learning-sample-problem.png)

$$
在这里，输入变量x是一个在空间R^2中的二维向量。x_1^{(i)}表示第i个房子的面积，x_2^{(i)}则表示第i个房子的卧室数量。
$$

我们可以用下面这个方程来近似表示这个例子中y关于x的线性关系：

$$
h_θ(x)=θ_0+θ_1x_1+θ_2x_2
$$

在这里，θ便被称之为参数（或者说权重（weight）），用于参数化空间X到Y的映射。有时候，我们可以将h<sub>θ</sub>(x)中的θ下标去掉，只要它不会引起不必要的误解。
为了更加简化我们的方程，我们还可以这样写：

$$
h(x)=\sum^n_{i=0}θ_ix_i=θ^Tx
$$

为了使h(x)（近似值）与y（实际输出值）尽可能的接近，我们需要一个函数用于描述h(x)与y之间的差距，或说误差，当函数值越小，h(x)就会越接近y值，这个函数，我们称之为**成本函数**：

$$
J(θ)=\frac{1}{2}\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})^2
$$

之所以使用**平方**，是为了避免正负误差相互抵消，前面的1/2则是为了方便我们之后的计算。

### 梯度下降

#### 标准公式

为了使J(θ)（误差）尽可能的小，我们需要很好的挑选θ的值，一般来说，我们在最初会给θ挑一个接近但是并不完全准确的值，然后，通过不断地调整，使θ一步一步地接近我们所期望的θ值。

为了实现这个目的，我们一般会采用**梯度下降**算法（gradient descent），它以一个初始的θ作为起始点，然后不断地更新θ的值，直到J(θ)成为最小值，如图：

![](img\gradient-descent.png)
[图片来源 梯度下降(Gradient Descent)小结 - 刘建平Pinard - 博客园](http://www.cnblogs.com/pinard/p/5970503.html)

从一个初始的θ值，一步一步沿着最快的下降方向使得j(θ)达到最小值。它每一步的计算的具体公式如下:

$$
θ_j:=θ_j-α\frac{\delta}{\delta θ_j}J(θ)
$$

在这里，α被称为**学习速度**，是一个由我们手动设置的算法参数。它表示每一次进行梯度下降时下降的幅度，如果过小会导致计算速度缓慢，过大会则导致一次性下降幅度过大以至于跳过了最小值。

而$$$\frac{\delta}{\delta θ_j}J(θ)$$$则表示J(θ)的偏导，我们知道，一个函数关于某点的偏导与该点构成的Nabla算子便是该函数在该点上的变化率最大（1）的方向向量，即梯度。

由此我们可以语言描述如下：

$$
更新后的参数=初始参数-学习速度*梯度
$$

要注意这里的参数也是向量。

#### 计算方式

只看上面的公式该如何执行或许还有些麻烦，因此我们还需要将公式展开，以便于了解公式的计算方式。

假设我们只有一个样本(x,y)，这样子我们可以忽略函数值J的总和，因此得到:

![](img\ml1-1.png)

然后在这个单一样本中，我们就可以进行如下更新：

![](img\ml1-2.png)

这个过程就被称为**最小均方更新法**（LMS(Least mean quares) update rule），这个地方我们可以直观的看到，误差J(θ)也会根据θ值的更新而变得越来越接近0，如果一开始选择的初始θ很大，那么J(θ)也会非常大。

我们重复更新θ值，直到J(θ)的变化变得非常小，从而获得我们的回归方程。

##### 批梯度下降算法

一般来说，我们都会这样子执行上述的计算过程：

重复按下面公式更新θ

![](img\ml1-3.png)

直到J(θ)收敛

这个计算方式被称为**批梯度下降算法**(batch gradient descent)，“批”字表明这个算法在θ中的每一次更新，都会对所有的样本总计m个样本进行计算，但也因此，我们会发现，一旦m的数量过于庞大，这个算法的计算量也会相应变得非常巨大，因此，为了能够对应大量数据，我们就需要使用更加适合真实数据集的算法：

##### 随机梯度下降算法（增量梯度下降算法）

它的计算方式如下：

![](img\ml1-5.png)

可以看到，在这个计算过程中，我们每次只从总样本中抽取一个样本来进行计算。这种计算方式就被称为**随机梯度下降算法**/增量梯度下降算法（stochastic gradient descent/incremental gradient descent）。但是，通过随机梯度下降算法往往只能够得到比较接近我们期望的θ的θ，

### 正规方程组

除了梯度下降，我们还可以使用正规方程组来得到我们想要的结果。这个方法通过矩阵的计算达到结果，这会比我们直接使用代数进行计算要好一些。

##### 矩阵梯度

首先我们来定义一下对以矩阵作为参数的函数的梯度运算：

![](img\ml1-6.png)

例子：
假设我们有函数

![](img\ml1-7.png)

则

![](img\ml1-8.png)

##### 设计矩阵

我们定义矩阵X为设计矩阵，它的表示如下：

![](img\ml1-9.png)

这个矩阵表示所有样本的输入。同时，我们再设定

$$
\vec{y}为m由所有目标值组成的m维向量
$$

![](img\ml1-13.png)

于是，我们作出如下方程：

![](img\ml1-12.png)

并由定理$$$z^Tz=\sum_iz^2_i$$$得到：

![](img\ml1-14.png)

##### 关于矩阵迹的几条性质

然后我们需要知道几条关于矩阵和迹的特殊性质：

![](img\ml1-10.png)

其中，第四条中的矩阵A必须为非奇异矩阵，第五条是由第二条和第三条推导出来的。

它们是由这几条性质推导而得：

如果A*B是方阵，则：

$$
trAB=trBA
$$

而且还可以根据上面得到：

$$
trABC=trCAB=trBCA\\
trABCD=trDABC=trCDAB=trBCDA
$$

如果A和B是方阵，并且a是一个实数：

$$
trA=trA^T\\
tr(A+B)=trA+trB\\
traA=atrA
$$

我们对最初五条性质进行一一推导：

**第一条**

$$
\nabla_AAB=B^T
$$

![](img\ml1-11.jpg)

**第二条**

$$
\nabla_{A^T}f(A)=(\nabla_Af(A))^T
$$

![](img\ml1-12.jpg)

**第三条**

$$
\nabla_AtrABA^TC=CAB+C^TAB^T
$$

![](img\ml1-11.png)

其实就是根据复合函数的求导法则与链式求导法则进行推导的。

![](img\ml1-14.jpg)

**第四条**

$$
\nabla_A|A|=|A|(A^{-1})^T
$$

$$
\nabla_A|A|=(adj(A))^T，adj(A)为A的伴随矩阵\\
=|A|(A^{-1})^T
$$

**第五条**

$$
\nabla_{A^T}trABA^TC=B^TA^TC^T+BA^TC
$$

由公式第二条和第三条求得，不再赘述

借由上面的公式，我们可以得到：

![](img\ml1-15.png)

备注：

* 因为
$$
θ^TX^TXθ-θ^TX^T\vec{y}-\vec{y}^TXθ+\vec{y}^T\vec{y}
$$
是一个实数，所以它的迹等于它本身

* 第四步，通过$$$trA=trA^T$$$将第二第三项合并
* 第五步他先把第一项化成
$$
trθ^TX^TXθI，然后利用第五条式子将其展开，其中\\
A^T=θ,B=X^TX,C=I
$$

最后，假设J(θ)为0，从而得到:

$$
X^TXθ=X^T\vec{y}\\
θ=(X^TX)^{-1}X^T\vec{y}
$$

这便是**正规方程组**，其实我们可以直接用矩阵的空间结构来得到这个方程，这在我的笔记[线性代数 最小二乘法](\_posts\blog\MachineLeraning\linear-algebra\2017-7-15-linear-algebra-least-square.md)有记录，这里就不再赘述。