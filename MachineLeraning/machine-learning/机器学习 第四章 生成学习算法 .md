# 机器学习 第四章 生成学习算法

资料来源参考：
[CS229 Machine Learning Course Materials Lecture notes 2](http://cs229.stanford.edu/notes/cs229-notes2.pdf)
[第五讲：生成学习算法、高斯判别分析、朴素贝叶斯算法](http://nbviewer.jupyter.org/github/zlotus/notes-LSJU-machine-learning/blob/master/chapter05.ipynb)

目前为止，我们在分类算法中仅了解了逻辑回归算法。举个例子，假如我有一群大象(y=1)和一群狗(y=0)，如果我们要对这两种动物应用逻辑回归算法，那么逻辑回归会试图根据它们的特征(x)以及它们的物种(y)在两种动物之间划出一条界限，用以区分二者，这时候，如果我们再给分类器一个它从未见过的动物（测试集），那么它就有一定概率判断出这个动物可能会是狗还是大象。

但实际上，我们还可以用另外一种方法来分类：
我们可以先对两种动物分别进行建模，对大象建立一个专门用于判断一个动物是否是大象的模型，对狗建立一个专门用于判断一个动物是否是狗的模型，完成之后，我们就可以拿一未知动物分别在这两个模型种测试，看看这个动物是更偏向于大象还是更偏向于狗。

对于前一类通过直接学习$$$p(y\mid x)$$$的算法(如逻辑回归)或者是从输入的特征值空间直接映射到结果空间0,1上的算法（如感知算法），通常被称为**判别**(discriminative)学习算法。而我们在例子中举得后一种，也是我们接下来将要介绍的算法并不对$$$p(y\mid x)$$$或者$$$p(y)$$$进行建模，它被称为**生成**(generative)算法。

## 生成学习算法

回到上面的例子，我们用y表示一个动物究竟是狗(y=0)还是大象(y=1)。那么模型$$$p(x\mid y=0)$$$就是狗的特征分布，模型$$$p(x\mid y=1)$$$就是大象的特征分布。

通过对$$$p(y)$$$（它被称为**类先验**(class priors)）和$$$p(x\mid y)$$$建模，我们可以用贝叶斯公式（讲义原文为贝叶斯公式，但这里看起来更类似于“乘法公式”？）来推导在条件x给出的情况下，y的后验概率：

$$
p(y \mid x)=\frac{p(x\mid y)p(y)}{p(x)}
$$

上式中的分母$$$p(x)$$$可以用全概率公式展开为

$$
p(x)=p(x\mid y=1)p(y=1)+p(x\mid y=0)p(y=0)
$$

实际上，如果是为了做出预测而需要知道$$$p(y\mid x)$$$，并不需要计算出分母的值，因为：

![](img\ml4-1.png)

$$$arg~max_y$$$表示使取后面跟着的式子最大值时y的取值，例如$$$arg~min_x(x-5)^2=5$$$。就表示$$$(x-5)^2$$$取最小值时x的取值，就是5了。

上式表明，当$$$p(x\mid y)p(y)$$$取最大值时它的参数$$$y$$$就是能使对应公式$$$p(y\mid x)$$$取值最大的$$$y$$$值。之所以这么说，是因为通过全概率展开式我们可以看出，$$$p(x)$$$的取值是$$$y=1时x$$$的概率和$$$y=0时x$$$的概率的加和，这说明$$$y$$$的取值不影响$$$p(x)$$$的取值，或者说，$$$p(x)$$$是初始决定而不在计算中影响这个式子的。另外，如果$$$y$$$服从均匀分布，即不论$$$y$$$值为什么$$$p(y)$$$都不变，那么我们还可以直接将式子简化为$$$arg~max_yp(y\mid x)=arg~max_yp(x\mid y)$$$

### 高斯判别法

我们要介绍的第一种生成学习算法是**高斯判别法**(GDA)。在这个模型中，我们假设$$$p(x\mid y)$$$是服从高斯分布的。不过，在将这个算法之前，我们还要先讨论一下**多元高斯分布**(the multivariate Gaussian distribution)。

#### 多元高斯分布

在n维上的**多元正态分布**（the multivariate normal distribution），也被称为多元高斯分布。它由一个**期望向量**(mean vector)$$$μ∈R^n$$$和一个**协方差矩阵**$$$\sum ∈ R^{n*n}$$$参数化，其中，$$$\sum ≥ 0$$$是一个对称半正定矩阵。这个分布也被写作$$$N(μ,Σ)$$$，它的概率密度函数为：

![](img\ml4-2.png)

在上面的方程中，$$$\mid Σ \mid$$$表示矩阵Σ的行列式。不出意料的是（因为单一随机变量的高斯分布也一样），随机向量X若服从$$$N(μ,Σ)$$$,则它的数学期望便是$$$μ$$$：

![](img\ml4-3.png)

我们知道，计算一个二元分布的协方差的公式是：

$$
cov(XY)=E[(X-E[X])(Y-E[Y])]
$$

由此我们推断计算多元分布中随机向量Z的协方差公式为：

$$
cov(Z)=E[(Z-E[Z])(Z-E[Z])^T]
$$

这个公式将单一实随机变量的方差一般化了。另外，方差也可以通过公式

$$
cov(Z)=E[ZZ^T]-(E[Z])(E[Z])^T
$$

来计算。（这个很容易证明，上式展开即可）

因此，如果有$$$X \sim N(μ,Σ)$$$那么，它的协方差：

$$
cov(X)=Σ
$$

下面这几张图形象地表示了二元高斯分布：

![](img\ml4-4.png)

最左边的图表示了期望为0(2x1向量)协方差矩阵为单位矩阵(即$$$Σ=I$$$，在这幅图协方差矩阵是个2x2矩阵)的高斯分布图像。期望为0协方差矩阵为单位矩阵的高斯分布又被称为**标准正态分布**(the standard normal distribution)。中间的高斯分布的协方差矩阵$$$Σ=0.6I$$$，而最右边那幅图$$$Σ=2I$$$。由此我们可以看出，Σ越大的，高斯分布也会越分散，反之则会越集中。

我们再来看另外三幅图：

![](img\ml4-5.png)

这几幅图的期望μ都是0，它们的协方差矩阵分别为：

![](img\ml4-6.png)

我们可以看到，通过增大协方差矩阵的副对角线上的元素，可以让图像沿$$$45^o$$$线(因为$$$x_1=x_2$$$)方向压缩。下面三幅等高线图可以更加清楚地表示这个变化（由于图形宽高比的原因第一幅图也像是椭圆，但实际上，它是一个正圆形）：

![](img\ml4-7.png)

而下面左边的两幅则是沿着$$$-45^o$$$方向的：

![](img\ml4-8.png)

它们的协方差矩阵分别为：

![](img\ml4-9.png)

最后，从右边的图我们可以看出来，改变协方差矩阵对角线上元素的值，等高线的密度也有所变化。

然后，我们可以看一看当$$$Σ=I$$$不变的时候，改变向量μ的效果：

![](img\ml4-10.png)

它们的期望向量分别是：

![](img\ml4-11.png)

#### 高斯判别模型

当一个分类问题，它输入的特征值x为连续的时候，我们就可以使用高斯判别模型(GDA),它通过对$$$p(x\mid y)$$$上建立多元正态分布来建模：

![](img\ml4-12.png)

将分布状况写出来，就是：

![](img\ml4-13.png)

在这里，模型的参数是$$$Φ,Σ,μ_0和μ_1$$$（要注意，尽管期望向量$$$μ_0$$$和$$$μ_1$$$不一样，这个模型也只会使用同一个协方差矩阵）。然后，这个数据的对数似然函数为：

![](img\ml4-14.png)

这个公式我们可以用乘法公式推出。

为了让l取得最大值，我们找到的各个参数的最大似然估计分别为：

![](img\ml4-15.png)

求解方法参考[problem set solution 1](http://cs229.stanford.edu/materials/ps1_key.pdf)

该算法用图像表示如下：

![](img\ml4-16.png)

在图中便是给两个类型分别建立的模型。我们注意到两个高斯判别模型具有相同的形状，这是因为它们具有相同的协方差矩阵。图中的直线便是由$$$p(y=1\mid x)=0.5$$$得到的判定边界了。在边界的一侧，样本将得到$$$y=1$$$的预测，在另一边这是$$$y=0$$$。

#### GDA和逻辑回归的关系

GDA模型和逻辑回归有一些很有意思的联系。如果我们将$$$p(y=1\mid x;Φ,μ_0,μ_1，Σ)$$$当作是关于x的方程，那么我们就可以发现有：

![](img\ml4-17.png)

其中θ是一个关于$$$Φ,Σ,μ_0,μ_1$$$的函数。这条式子事实上就是我们的逻辑回归中的sigmoid函数，用于对$$$p(y=1\mid x)$$$建模。

那么，我们该如何在这两个模型之间做出合理地选择呢？而且，这两个模型哪怕是在相同的训练集中也一般会产生不同的效果。

我们之前讨论过，如果$$$p(x\mid y)$$$是多元高斯分布（各元变量使用同一Σ），那么$$$p(y\mid x)$$$就会是一个sigmoid函数。但反过来则不一定。如果$$$p(y\mid x)$$$是一个逻辑函数，并不能推导出$$$p(x\mid y)$$$是多元高斯分布。这说明，相对于逻辑回归，高斯判别分析对数据做出了更强的模型假设（我们假设了在y条件下x服从高斯分布，这是一个很强的假设）。也就是说，在假设模型正确(或y | x大致服从高斯分布)的前提下，同逻辑回归相比，高斯判别法能够对数据做出更好的拟合，得到更好的模型（即我们显示认定训练集数据服从高斯分布，因此如果我们在算法中使用了这个假设，那么算法的表现就会更好，因为这个算法利用了更多关于数据的信息，算法预先就“知道”数据服从高斯分布）。
特别是，当$$$p(x\mid y)$$$确实大致服从多元高斯分布时，高斯判别法是**渐进有效**(asymptotically efficient)的。非正式地说，这意味着当训练集很大的时候，几乎没有什么算法能够比GDA更精确。所以哪怕是在一个小的训练集上，我们也往往会采用GDA，因为我们做出了足够强的假设，足矣让高斯判别法通过极少量地数据就得到结果。

但从另一方面来说，对于很弱的假设，逻辑回归更加“健壮”。它对异常模型的假设更加不敏感。许多不同的假设最终都能以逻辑函数的形式推导出$$$p(y\mid x)$$$。

所以，如果数据实际上不符合高斯分布，那么高斯判别法的效果就会非常的不可靠。（因此，若不确定$$$x\mid y$$$的分布，那么逻辑回归的表现可能会更好，因为它预先做出的假设比较少，不过相应的，与高斯模型相比它也会需要更多的训练样本）

（实际上，更一般的，当$$$x\mid y$$$服从指数分布族中的任意分布，$$$p(y\mid x)$$$都会是一个逻辑函数）

综上，当模型假设正确或近似正确时，高是判别分析能够做出更强的模型假设，且对训练数据效率更高（使用少量样本即能达到效果）。而逻辑回归假设较弱，对偏离型模型的数据表现出良好的健壮性（更抗噪音）。因此，若有明显不属于高斯分布，并且有较大训练集的情况，逻辑回归会更好一点。