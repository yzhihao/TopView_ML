# 机器学习 第五章 朴素贝叶斯算法(Naive Bayes) 拉普拉斯平滑(Laplace smoothing) 文本分类的事件模型

[CS229 lecture notes 2](http://cs229.stanford.edu/notes/cs229-notes2.pdf)

[生成学习算法、高斯判别分析、朴素贝叶斯算法](http://nbviewer.jupyter.org/github/zlotus/notes-LSJU-machine-learning/blob/master/chapter05.ipynb)

[事件模型、函数间隔与几何间隔](http://nbviewer.jupyter.org/github/zlotus/notes-LSJU-machine-learning/blob/master/chapter06.ipynb)

## 朴素贝叶斯

### 文本分类问题

在我们的高斯判别法中，输入特征x的值是连续的实数向量。现在，我们来讨论另外一种算法，它的输入特征是离散的。

首先我们来举一个例子：
假设我们要建立一个垃圾邮件分类器，它能通过邮件的内容有效地鉴别出这封邮件是不是垃圾邮件。这种对文本内容进行分类的问题，就叫做**文本分类**(text classification)

现在我们假设我们有一个训练集，它由一些已经被分类标注为“垃圾”或者“非垃圾”的邮件组成。藉由这个训练集，我们将从识别“垃圾”邮件何“非垃圾”邮件的输入特征来构建我们的分类器。

首先我们要从这些邮件中提取出一个由词典（这个词典一般由我们遍历训练集得到，也有通过遍历英文词典得到的，不过比较少）中所有词构成的特征向量，它的长度是词典中所有词汇的数目总和。然后我们规定，如果词典中的第i个词出现在邮件中，那么就设$$$x_i=1$$$；否则，$$$x_i=0$$$。

这个向量的具体形象如图

![](img\ml5-1.png)

这个向量表示一封邮件中出现的词语有哪些，像是"a"或者"buy"出现了，而"aardvark","aardwolf","zygmurgy"就没有出现。这些被编码在特征向量中的词集被称为**词汇表**(vocabulary)。因此x的维数就由词汇表的长度决定。

### 朴素贝叶斯假设

在选择好我们的特征向量之后，我们就要对$$$p(x\mid y)$$$建模。但是，如果我们像之前一样建模，那么如果一个词汇表包含了50000个单词，我们的输入特征x也会是一个50000维的值为1和0的向量，而因此我们也会得到一个由$$$2^{500000}-1$$$个参数组成的参数向量，那样子显然会占用过多的空间。（要注意的是，为了节省内存我们往往会精心设计词汇表，将一些使用频率非常高，对文章影响非常小的词（如：“a”，“the”，“for”）排除（这些词几乎会出现在所有文章中，我们称其为**stop words**））

因此，在给$$$p(x\mid y)$$$建模之前，我们还需要给它做一个强假设以减少我们建模的工作量，这个假设如下：

$$
所有的x_i对于给定条件y是相互独立的
$$

需要注意的是这个假设的意思不是指所有$$$x_i$$$相互独立，而是指$$$x_i\mid y$$$相互独立，即$$$p(x_i\mid x_k, y)=p(x_i\mid y)$$$。这个假设我们称之为**朴素贝叶斯假设**(Naive Bayes(NB) assumption)，而据此假设得到的算法便被称为**朴素贝叶斯分类器**。为了解释清楚这个假设，我们再举个例子：
假设在词典中“buy”是第2087个词，"price"是第39831个词，那么当我们已知一封邮件是垃圾邮件（y=1）时，"buy"在邮件中出现与否（即$$$x_{2087}$$$的值）不影响"price"（即$$$x_{39831}$$$的值）。这个例子也可以被写作：$$$p(x_{2087}\mid y)=p(x_{2087}\mid y,x_{39831})$$$（注意不是$$$p(x_{2087})=p()x_{2087}\mid x_{39831}$$$）。在这个假设成立的条件下（尽管我们知道这是一个很强的假设，因为哪怕邮件被标为垃圾邮件，“buy”和“price”也不见得就真的毫无关系，事实上往往出现了其中一个，另外一个也出现的概率就会更大。但是，事实又表明，这个假设可以让朴素贝叶斯算法的运作效果非常好），我们就有了下面这个公式：

![](img\ml5-2.png)

（该公式通过联合分布的链式法则推导，推导过程大致如下：

$$
\because p(a\mid b)p(b)=p(ab)\\
\therefore p(x_1\mid y)p(x_2\mid y,x_1)...p(x_n\mid y,x_1...x_{n-1})\\
=p(x_1,x_2\mid y)p(x_3\mid y,x_1,x_2)...p(x_n\mid x_1,...,x_{n-1})\\
=p(x_1,...,x_n\mid y)
$$

）

### 朴素贝叶斯算法的参数化与极大似然估计

我们用$$$Φ_{i\mid y=1}=p(x_i=1\mid y=1)$$$，$$$Φ_{i\mid y=0}=p(x_i=1\mid y=0)$$$和$$$Φ_y=p(y=1)$$$来参数化我们的模型。然后和之前一样，我们假设我们有训练集$$$\{(x^{(i)},y^{(i)});i=1,...,m\}$$$，这样子，我们就可以得到联合似然函数：

![](img\ml5-3.png)

求L有关于$$$Φ_y,Φ_{i\mid y=0},Φ_{i\mid y=1}$$$的最大值时，我们得到极大似然估计：

![](img\ml5-4.png)

（讲义原文如此，但是我们发现，各个参数的似然估计似乎可以直接通过联合概率的知识得到）

在上面那个方程中，符号"$$$\bigwedge$$$"表示“并且”。这些参数符号都有非常自然的含义，例如，$$$Φ_{j\mid y=1}$$$就表示垃圾邮件中词语$$$x_j$$$出现的概率。

在得到所有的参数拟合后（这里求参数拟合的方式就是将训练集的样本代入到上面各个参数的极大似然估计中，那样我们就可以让每个参数得到一个具体的值），我们就可以对一个新的具有特征x的邮件是垃圾邮件的概率进行预测：

![](img\ml5-5.png)

然后选择后验概率较大者作为预测结果。

### 离散化

但是，我想我们应该注意到，这里的特征$$$x_i$$$只能是一个二元特征（0或1）。因此，我们接下来介绍一个适用于一般化的特征值$$$x_i$$$能够在1,2,...,k取值的算法。

要实现这个算法我们将$$$p(x_i\mid y)$$$的模型从伯努利分布改为多项分布即可。事实上，哪怕输入特征值是连续的，我们也可以将其**离散化**(discretize)，通过把它们分成一系列有限个的离散值，来创建能够使用朴素贝叶斯法的条件。

举个例子，假设我们想用特征值$$$x_i$$$来表示住房面积，那么我们可以将连续的住房面积离散化成下表：

![](img\ml5-6.png)

这样子，像是面积为890平方英尺的房子我们就可以将它的特征值$$$x_i$$$设为3.

接下来，我们就可以应用朴素贝叶斯算法，然后$$$p(x_i\mid y)$$$代入多项分布。

因此，当原始值为连续值以至于我们无法对其应用多项分布时，我们可以将其离散化，然后对其应用朴素贝叶斯算法（而不是高斯判别法）往往可以得到不错的结果。

## 拉普拉斯平滑

朴素贝叶斯算法的应用已经可以有效地解决很多问题，但我们还可以再给它做一点微小的改动以让它能够变得更加完善。

### 引子

假设我们收到了一封新的邮件，然后这个邮件中包含了一个我们的垃圾分类器在训练样本中没有见过的词，假设这个词是"NIPS"（这个词我沿用了讲义的假设，NIPS是世界最顶尖的机器学习会议之一），并且这个词是我们词典中的第35000个词，那么我们用朴素贝叶斯分类器对它做出的极大似然估计值为：

![](img\ml5-7.png)

很显然，这是因为$$$x^{(i)}_{35000}=1$$$这个值必然为0，因为它根本没有在训练集中出现过。也正因此，我们在计算一封包含这个词的邮件为垃圾的概率就会变成：

![](img\ml5-8.png)

但这显然是不合理的（$$$\frac{0}{0}$$$很明显是一个错误的预测）。我们也不能粗暴地就将这个概率理解为0，就像是，lpl年年0:3输给lck我们也不能说他以后对战lck的胜率也只能是0了（事实上，今年亚洲赛的时候lck还真就获得了冠军）。

### 解决方案

要解决这个问题，我们可以先看一下在$$$\{1,...,k\}$$$中取值的多项随机变量z的期望估计：我们使用$$$Φ_i=p(z=i)$$$参数化这个多项分布，对于有m个独立观测集合$$$\{z^{(1)},...,z^{(i)}\}$$$，参数的极大似然估计为：

![](img\ml5-9.png)

然后，为了解决前面我们提到的那个问题，我们对其应用**拉普拉斯平滑**(Laplace smoothing)，将上面的估计替换为：

![](img\ml5-10.png)

我们给分子加了1，分母加了k（随机变量z所能取的最大值），这样子$$$\sum^k_{j=1}Φ_j=1$$$仍然成立（其和必须为1，因为$$$Φ_j$$$是各可能结果的概率估计）；同时，对于所有$$$Φ_j都有Φ_j≠0$$$成立。

在特定条件下（尚有争议？），拉普拉斯平滑给了参数$$$Φ_j$$$一个理想的估计。

现在，我们来将拉普拉斯平滑应用到我们的朴素贝叶斯分类器中，我们对朴素贝叶斯算法中参数的极大似然估计做出如下修改：

![](img\ml5-11.png)

（对$$$Φ_y$$$用不用拉普拉斯平滑都没有所谓，因为训练集中，对于正常或垃圾邮件的概率p(y=1),p(y=0),都不太可能会是0值）

## 文本分类的事件模型

我们知道朴素贝叶斯算法可以很多分类问题中效果都不错，但实际上，对于文本分类这一块，我们还有比朴素贝叶斯算法更强的模型。

在特定语境的文本分类中，朴素贝叶斯使用了**多元伯努利事件模型**(multi-variate Bernoulli event model)（特征值按照伯努利试验取$$$x_i∈\{0,1\}$$$，特征值向量长度为字典长度）。在该模型中，我们假设下一封邮件的发送方已经确定是一个随机的邮件发送人（可能是垃圾邮件制造者，也可能只是普通的联系人，这是由我们确定的条件概率中的概率）；然后我们还假设，发送者会遍历词典，然后决定是否将单词$$$x_i$$$写入邮件（该词语在邮件类别（垃圾或非垃圾）确定的情况下相对于其它词语独立），并且该词语服从概率$$$p(x_i\mid y)=Φ_{i\mid y}$$$。于是我们可以得到$$$p(x\mid y)=\prod^n_{i=1}p(x_i\mid y)$$$和$$$p(y)$$$，再根据

$$
arg~max_yp(y\mid x)=arg~max_yp(x\mid y)p(y)
$$

最终得到该邮件是垃圾邮件的概率：

$$
p(y)\prod^n_{i=1}p(x_i\mid y)
$$

而我们下面将要介绍的模型是朴素贝叶斯算法的简单变形，它被称为**多项事件模型**(multinomial event model)。

### 符号规定

为了描述这个模型，我们首先需要引入一些不同的符号和特征用于代表邮件：

1. 我们规定$$$x_i$$$是邮件中的第i个词。
2. $$$x_i$$$从集合$$$\{1,...,\mid V\mid\}$$$中取值，$$$\mid V\mid$$$是词汇表（词典）的长度。
3. 一封有n个词的邮件现在由长度为n的向量($$$x_1,x_2,...,x_n$$$)来表示

举个例子，如果一封邮件的开头是"A NIPS ..."那么表示这个邮件的向量的前两个元素便分别是$$$x_1=1$$$，$$$x_2=35000$$$（记得我们的假设是"a"是词汇表的第一个词，“NIPS”是词汇表的第35000个词）

### 假设与推导

在多项事件模型中，我们假设邮件发送方已经由一个随机过程确定（即根据p(y)），可能是垃圾邮件制造者也可能是普通联系人；然后，发送者会根据多项分布选出词语$$$x_1$$$（这个概率是$$$p(x_1\mid y)$$$），再选出$$$x_2$$$...以此类推直到选出n个词构成一封完整的邮件。因此，在这个假设中这封邮件是垃圾邮件的概率是

$$
p(y)\prod^n_{i=1}p(x_i\mid y)
$$

注意到这个方程和我们前面说的多项伯努利事件分布很相似，但事实上这个方程和前面的那个方程区别是非常大的。特别是$$$x_i\mid y$$$现在是一个多项分布，而不是伯努利分布。

我们新模型里面的参数和之前还是一样的:$$$Φ_y=p(y)$$$,$$$Φ_{k\mid y=1}=p(x_j=k\mid y=1)$$$以及$$$Φ_{i\mid y=0}=p(x_j=k\mid y=0)$$$。注意到，我们假设不论j为何值$$$p(x_j\mid y)$$$的值都是一样的。（意思就是说，这个词语在邮件中出现的位置不造成影响）

### 求极大似然

现在我们假设我们有训练集$$$\{(x^{(i)},y^{(i)});i=1,...,m\}$$$并且$$$x^{(i)}=(x_1^{(i)},x_2^{(i)},...x_{n_i}^{(i)})$$$(这里$$$n_i$$$表示在第i个样本中词语的数量)，由此，我们可以得到似然函数：

![](img\ml5-12.png)

然后得到极大似然估计：

![](img\ml5-13.png)

我们还可以再对它应用拉普拉斯平滑：

![](img\ml5-14.png)

（即在分子加1，分母加$$$\mid V\mid$$$）

在处理文本类问题的时候，多项事件模型往往比原始的朴素贝叶斯算法效果更好，一个原因就是因为它考虑了每个单词出现的次数。

尽管朴素贝叶斯分类器不是最好的分类算法，但它的效果却往往非常好，再加上简单并且易于实现的特性，我们通常称它为“首选试验算法”（使用朴素贝叶斯算法最终也会得到一个逻辑函数形式的后验分布，也即是说，朴素贝叶斯算法也属于指数分布族，他仍然是一个线性分类器。）