---
layout:     post
title:      "机器学习基础"
subtitle:   ""
date:       2017-07-18 18:00:00
author:     "Wes"
header-img: "img/机器学习基础-bg.jpg"
catalog: true
tags:
    - Machine Learning
---
# 机器学习基础

## 过拟合和欠拟合

**过拟合(overfitting)：指学习能力过强，过度学习了训练数据中的细节和噪音，模型对于训练数据拟合程度过当的情况，往往对于训练数据变现很好，但是泛化能力差**  
**欠拟合(underfitting)：欠拟合就与过拟合相反，没能将训练样本的特征学习好，在训练集和测试集都变现不好**  
**对于欠拟合一般比较容易发现，而过拟合就需要一些方法**
![](https://aswz.github.io/assets/img/机器学习基础/欠拟合和过拟合.png)  
这就是欠拟合和过拟合的变现，左边的曲线为欠拟合，右边的曲线为过拟合，中间是比较合适的模型  
**可以发现过拟合，欠拟合和模型复杂度是相关的**  
**偏差(bais)：当模型做出与实际情况不符的假设时就会引起错误，这种错误称为偏差**，就是预测变量与因变量之间的关系差别太大(通常是模型太简单)，就会发生偏差，即模型“偏差”向一个错误的方程  
**方差(variance)：方差是由一种由于训练数据集的波动(fluctuations)引起的错误**，就是当学习算法随着训练数据集的不同而呈现出太大的波动时，所引起的错误就称为方差  
方差和偏差具有矛盾性，这就是常说的偏差-方差窘伪境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大  

## 模型选择和评估方法

在对学习器进行评估时，往往需要一个“测试集”(testing set)，来测试学习器对新样本的判别能力，然后以测试集上的“测试误差”(testing error)作为泛化误差的近似，**需要注意测试样本尽量与训练集互斥**，下面几种选取测试集的方法  

- **留出法(hold-out)**直接将数据集**D**划分为两个互斥的集合，其中一个集合作为训练集**S**，另一个作为测试集**T**，即**D=S∪T，S∩T=Ф**，在**S**上训出模型，用**T**来评估其测试误差，作为对泛化误差的估计，**常取2/3~4/5用于训练，剩余样本用于测试**  
- **交叉验证法(cross validation)**先将数据集**D**划分为**k**个大小相似的互斥子集，即**D=D<sub>1</sub>∪D<sub>2</sub>∪...∪D<sub>k</sub>，D<sub>i</sub>∩D<sub>j</sub>=Φ(i≠j)**，然后每次用**k-1**个子集的并集作为训练集，余下的那个子集作为测试集，就有**k**组训练/测试集，进行**k**次训练和测试，最终返回的是这**k**个测试结果的均值，**通常把交叉验证法称为“k折交叉验证(k-fold cross validation)”**，**k最常取10，也有取5，20的时候**  
- **留一法(LOO)**就是一种极端的k折交叉验证法，取k=m的时候  
- **自助法**，就是在**m**个样本的数据集**D**，我们对它进行采样产生数据集**D'**：每次随机从**D**中挑选一个样本，将其拷贝放入**D'**，然后将其放回**D**中，这个过程重复执行**m**次，而有一部分的样本是始终不出现的，去极限的，**lim<sub>m->∞</sub>(1-1/m)<sup>m</sup>->1/e≈0.368**，说明在初始数据集**D**中约有**36.8%**的样本未出现在采集数据集**D'**中出现  

**自助法在数据集较小，难以有效划分训练/测试集时很有用，然而自助法产生的数据集改变了初始数据集的分布，会引入估计偏差，在初始数据量足够时，留出法和交叉验证法更常用**  

## 查准率、查全率与F1

对于二分类问题，**查准率(precision)和查全率(recall)**，形象的说就是诊断病人时，**查准率就是正确诊断病人个数的占所有诊断为病人个数的比例**，**查全率就是正确诊断病人个数占全部病人的个数**  
![](https://aswz.github.io/assets/img/机器学习基础/分类结果混淆矩阵.jpg)
**查准率P=TP/(TP+FP)**，**查全率R=TP/(TP+FN)**  
**查准率和查全率是一对矛盾的度量，一般来说查准率高时，查全率往往偏低，而查全率高时，查准率往往偏低**  
**P-R图直观地显示出在样本总体上的查全率、查准率**  
![](https://aswz.github.io/assets/img/机器学习基础/P-R曲线.png)  
**可以计算P-R曲线下面积来比较曲线性能**，由于该值通常不太容易计算，有时候使用**“平衡点”**来判断，然而这又太简单了点，更常用的是**F1度量**，**F1=(2\*P\*R)/(P+R)=(2\*TP)/(样例总数+TP-TN)**  
还有更一般化的形式-**F<sub>β</sub>=((1+β<sup>2</sup>)\*P\*R)/((β<sup>2</sup>\*P)\*R)**，**当β等于1时，就相当于查全率和查准率有相同的影响，β>1查全率有更大的影响，β < 1时查准率有更大影响**  

## ROC和ACU

**ROC曲线**是以**“真正例率(TPR)”**为纵坐标，**“假正例率”(FPR)**，**TRP=TP/(TP+FN)**，**FPR=FP/(TN+FP)**  
**如何画出ROC曲线**，对于一个特定的分类器和测试数据集，显然只能得到一个分类结果，即一组FPR和TPR结果，而要得到一个曲线，我们实际上需要**一系列FPR**和**TPR**的值才能得到这样的曲线，这又是如何得到的呢？  
可以通过分类器的一个重要功能“概率输出”，即表示分类器认为某个样本具有多大的概率属于正样本（或负样本），来动态调整一个样本是否属于正负样本，例如  
![](https://aswz.github.io/assets/img/机器学习基础/ROC曲线绘制前阈值.png)  
以此取不同的**“Score”**值作为阈值，当**大于或等于**时作为正样本，**小于**时作为负样本，可以得到下面曲线  
![](https://aswz.github.io/assets/img/机器学习基础/ROC曲线样例.png)  
对于分类器而言越靠近左上角，效果越好，而往往会用**ROC曲线**的面积**ACU**来判断分类器的好坏  
对于为什么要选用**ROC曲线**去评价，因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化，下面是**PR曲线**和**ROC曲线**的比较  
![](https://aswz.github.io/assets/img/机器学习基础/ROC优势.jpg)  
在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大  