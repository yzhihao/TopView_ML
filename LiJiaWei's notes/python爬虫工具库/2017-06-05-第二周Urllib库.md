
* [Urllib库的使用](#Urllib库的使用)
	* [Urllib库介绍](#Urllib库介绍)
	* [Opener和Handler](#Opener和Handler)
	* [Request](#Request)
	* [POST与GET](#POST与GET)
		* [POST](#POST)
		* [GET](#GET)
	* [Headers设置](#Headers设置)
	* [Proxy（代理）设置](#Proxy（代理）设置)
	* [Timeout设置](#Timeout设置)


<div id="Urllib库的使用"></div>

# Urllib库的使用

<br />

<div id="Urllib库介绍"></div>

## Urllib库介绍

urllib2是Python的一个**获取URL**(Uniform Resource Locator)的工具库。提供各种函数接口来获取网页中的资源  
**使用前需要先导入**  
```python
import urllib2

url = 'https://www.baidu.com'
response = urllib2.urlopen(url)
print response.read()
```
`urlopen(url, data, timeout)`使用该函数可以简单获取网页源码  
参数： **url** 统一资源定位符(即网址)，**data**是访问url时要传送的数据，**timeout**是设置超时时间，后两个参数为默认参数。  
返回一个相关请求response对象，可以调用该对象的`.read()`来读取网页源码  

<br />

<div id="Opener和Handler"></div>

## Opener和Handler

**opener是OpenerDirector操作类的一个实例，OpenerDirector是一个管理很多处理类（Handler）的类**  
**而所有这些 Handler 类都对应处理相应的协议，或者特殊功能**  
我们先来看看`urlopen()`的源码
```python
_opener = None
def urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):
    global _opener
    if _opener is None:
        _opener = build_opener()
    return _opener.open(url, data, timeout)
```
这里我们可以看到，使用`urlopen()`生成的是我们前面提到的管理很多处理器类的**OpenerDirector操作类**，然后给他加入很多的处理器,作为其一个属性，然后调用该处理器操作类对象的`.open()`方法就可以获取页面了  
当然我们也可以用`build_opener()`的方法添加handler构建opener，用`install_opener()`设置urllib2 的**全局opener**  
```python
import urllib2

url = 'https://www.baidu.com'
request = urllib2.Request(url)
opener = urllib2.build_opener()
response = opener.open(request)
print response.read()
```
**虽然这个例子和直接调用**`urlopen()`**方法没什么区别，但当你需要加其他handler的时候就有用了**  

<br />

<div id="Request"></div>

## Request

**HTTP建基于请求(request)和响应(response)-客户端制造请求服务器返回响应**  
其实在上面调用`urlopen()`的时候将构造Reqeust对象的过程隐去了，可以先构造Request对象在调用`urlopen()`函数  
```python
import urllib2

url = 'https://www.baidu.com'
request = urllib2.Request(url)
response = urllib2.urlopen(request)
print response.read()
```

<br />

**在构造Request对象时可以传入url，data等参数** 
这运行结果和上面的结果完全一样，只是多了个Request对象，**推荐用下面的写法**，在构造Request对象的时候可以加入更多内容，服务器响应请求得到应答，显得逻辑更清晰明确  

<br />

<div id="POST与GET"></div>

## POST与GET

在HTTP请求的时候，往往很多时候要动态的传递参数，以获取对应的响应  
这时候常用的就有**POST请求**和**GET请求**  
POST请求和GET请求的区别在于GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&相连，POST则不会在网址上显示所有的参数  
需要引用**urllib库**把表单的数据以标准的方式encode  

<div id="POST"></div>

### POST

利用POST请求模拟登陆学校教务系统  
```python
import urllib
import urllib2

url = 'http://222.200.98.147/login!doLogin.action'
values = {}
values['account'] = '3116004779'
values['pwd'] = 'xxxxxx'
values['verifycode'] = ''
data = urllib.urlencode(values)
request = urllib2.Request(url, data=data)
response = urllib2.urlopen(request)
print response.read()
```

<div id="GET"></div>

### GET

**GET可以用于访问也可以用于传送数据**  
利用GET请求模拟登陆学校教务系统  
```python
import urllib
import urllib2

url = 'http://222.200.98.147/login!doLogin.action'
values = {}
values['account'] = '3116004779'
values['pwd'] = 'xxxxxx'
values['verifycode'] = ''
data = urllib.urlencode(values)
geturl = url + '?' + data
request = urllib2.Request(geturl)
response = urllib2.urlopen(request)
print response.read()
```

<br />

**在data的数据内容不多，请求容易模拟时，经常使用**  

<br />

<div id="Headers设置"></div>

## Headers设置

**在爬取网页的时候往往需要伪装HTTP报头，其中就包括有Headers的设置**  
在打开浏览器的网络监听时，可以发现在网络加载的时候往往会发送大量的请求，在查看某个请求的时候可以在Headers中查看到请求的各种信息，文件编码、压缩方式、请求的agent等等  
**其中，agent就是请求的身份，如果没有写入，那么服务器不一定会有响应，所以往往需要设置Haeaders中agent**  
**不只是agent往往还要设置指向自己的referer来对付”反盗链”**
下面例子就是设置的格式  
```python
import urllib2

url = 'http://www.xxxxx.com/'
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
headers = { 'User-Agent' : user_agent
			'Referer':'http://www.xxxxx.com'}
request = urllib2.Request(url, headers=headers)
response = urllib2.urlopen(request)
page = response.read()
```

<br />

这样就可以在构建Request的时候设置Headers  

<br />

<div id="Proxy（代理）设置"></div>

## Proxy（代理）设置

**urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy**  
在访问网页的时候，有些网站会检测一段时间一个ip的访问次数，如果访问过于频繁，将会被限制访问，在这个时候就要使用**代理**来进行访问  
在使用代理的需要先实例化一个**ProxHandler处理类**，然后调用**build_opener类**产生我们的**opener管理器**对象，调用**管理器的open方法**，就能获取网页内容了,**opener管理器和handler处理类前面有讲述**  
```python
import urllib2

handler = urllib2.ProxyHandler(proxies = {'http' : 'http://some-proxy.com:8080'})
opener = urllib2.build_opener(handler)
response = opener.open('http://www.baidu.com/')
print response.read()
```

<br />

<div id="Timeout设置"></div>

## Timeout设置

在`urlopen()`还可以设置timeout参数，可以设置等待多久超时，可以避免因为网站响应过慢造成的影响  
因为timeout参数在data参数后面没有输入data参数时，**要写明形参，我建议写的时候都带上形参，避免不必要错误**  
```python
import urllib2

url = 'http://www.baidu.com'
response = urllib2.urlopen(url, timeout=10)
print response.read()

request = urllib2.Request(url)
response = urllib2.urlopen(request, timeout=10)
print response.read()
```

<br />

若访问超时会抛出`urllib2.URLError: <urlopen error timed out>`异常

